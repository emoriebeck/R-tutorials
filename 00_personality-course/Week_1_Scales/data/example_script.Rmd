---
title: "R Notebook"
date: "`r Sys.setlocale('LC_TIME', 'C'); format(Sys.time(), '%d\\\\. %B %Y')`"
output:
  html_document:
    theme: united
    highlight: tango
    df_print: paged
    code_folding: show
    toc: true
    toc_float: true
    toc_depth: 2
editor_options: 
  chunk_output_type: console
---

# What is Personality? (And how do we measure it?)  
This week, we are starting with the most basic question: what is personality. To answer this question, we will start with a psychometric approach as this is the dominant perspective in the field at present. 

My favorite definition of personality is that "personality is what personality tests test," which means that what we measure in personality tests defines what we think of personality as. Much of the field of personality focuses on how personality can be measured using inventories of items that describe what you generally do. In this exercise, we will address the ways that personality scientists and psychometricians typically create these inventories. The concepts we will test are defined below.  

# Workspace
## Packages
```{r}
library(psych)
library(plyr)
library(tidyverse)
```

## Data
First, we need to load in these data. We're going to use 3 data sets of different size to demonstrate some of these concepts. The items come from the International Personality Item Pool (IPIP) and are subset of responses from the [SAPA Project](https://sapa-project.org/).  

### IPIP20
```{r}
# load data
ipip20 <- read_csv("~/Dropbox (Brown)/Summer 2018/RA Files/Week 1 Scales/data/ipip20.csv")
```

### IPIP50  
```{r}
# load data
ipip50 <- read_csv("~/Dropbox (Brown)/Summer 2018/RA Files/Week 1 Scales/data/ipip50.csv")
```

### IPIP100
```{r}
# load data
ipip100 <- read_csv("~/Dropbox (Brown)/Summer 2018/RA Files/Week 1 Scales/data/ipip100.csv")
```

# Inter-Item Correlations  
The idea of inter-item correlations involves 2 concepts, coherence and differentiation. Items in the same scale should be related to one another and not related to items in other scales. To test this, we will use the `cor()` function  in R to calculate the correlation matrix of items in a scale.

Below, I've added code to do pull out the itmes from the Agreeableness scale then to use that to calculate the correlations among those items. Run the code below and look at the results. Then do so for Extraversion, Conscientiousness, EmotionalStability, and Intellect.  

## IPIP20
```{r}
a <- ipip20 %>% select(contains("Agreeableness"))
c <- ipip20 %>% select(contains("Conscientiousness"))
e <- ipip20 %>% select(contains("Extraversion"))
n <- ipip20 %>% select(contains("EmotionalStability"))
o <- ipip20 %>% select(contains("Intellect"))

(r_a <- cor(a, use = "pairwise"))
(r_c <- cor(c, use = "pairwise"))
(r_e <- cor(e, use = "pairwise"))
(r_n <- cor(n, use = "pairwise"))
(r_o <- cor(o, use = "pairwise"))
```


## Reverse Coding
Did you notice anything odd? If you remember from your reading, psychometricians often use items that capture the opposite of the construct under investigation to control for social desirability (sort of).  

The code below will load in the keys (positive or negative) for each of the data sets (data frames called `ipip20_items`, `ipip50_items`, and `ipip100_items`). I demonstrate reverse coding for the ipip20 data set. Your challenge is to also do so for the ipip50 and ipip100.  

```{r}
load("~/Dropbox (Brown)/Summer 2018/RA Files/Week 1 Scales/data/keys.RData")

keys20 <- ipip20_items$rev_code
keys50 <- ipip50_items$rev_code
keys100 <- ipip100_items$rev_code

ipip20[5:24] <- reverse.code(ipip20[5:24], keys = keys20)
ipip50[5:54] <- reverse.code(ipip50[5:54], keys = keys50)
ipip100[5:104] <- reverse.code(ipip100[5:104], keys = keys100)
```

Now that we've reverse coded the items, we can recalculate the correlations. They should make more sense now.  

## Back to IPIP20  
Do the same we did above for the remaining Big 5 traits for the IPIP20. Then do so for the IPIP50 and IPIP100.  

```{r}
a20 <- ipip20 %>% select(contains("Agreeableness"))
c20 <- ipip20 %>% select(contains("Conscientiousness"))
e20 <- ipip20 %>% select(contains("Extraversion"))
n20 <- ipip20 %>% select(contains("EmotionalStability"))
o20 <- ipip20 %>% select(contains("Intellect"))

(r_a20 <- cor(a20, use = "pairwise"))
(r_c20 <- cor(c20, use = "pairwise"))
(r_e20 <- cor(e20, use = "pairwise"))
(r_n20 <- cor(n20, use = "pairwise"))
(r_o20 <- cor(o20, use = "pairwise"))
```

## IPIP50
```{r}
a50 <- ipip50 %>% select(contains("Agreeableness"))
c50 <- ipip50 %>% select(contains("Conscientiousness"))
e50 <- ipip50 %>% select(contains("Extraversion"))
n50 <- ipip50 %>% select(contains("EmotionalStability"))
o50 <- ipip50 %>% select(contains("Intellect"))

(r_a50 <- cor(a50, use = "pairwise"))
(r_c50 <- cor(c50, use = "pairwise"))
(r_e50 <- cor(e50, use = "pairwise"))
(r_n50 <- cor(n50, use = "pairwise"))
(r_o50 <- cor(o50, use = "pairwise"))
```

## IPIP100
```{r}
a100 <- ipip100 %>% select(contains("Agreeableness"))
c100 <- ipip100 %>% select(contains("Conscientiousness"))
e100 <- ipip100 %>% select(contains("Extraversion"))
n100 <- ipip100 %>% select(contains("EmotionalStability"))
o100 <- ipip100 %>% select(contains("Intellect"))

(r_a100 <- cor(a100, use = "pairwise"))
(r_c100 <- cor(c100, use = "pairwise"))
(r_e100 <- cor(e100, use = "pairwise"))
(r_n100 <- cor(n100, use = "pairwise"))
(r_o100 <- cor(o100, use = "pairwise"))
```


## Average Inter-Item Correlations  
Now that we've looked at individual item level correlations, the next step is to look at them on average, with the idea that items within a scale should have similar relationships to one another and be high on average.  

Below, you see code to first remove the lower triangle of the correlation matrix and the diagonal to remove redundancy in the correlation matrix of Agreeableness (since correlation matrices are symmetric). Then you see code to take that matrix and calculate its mean.  


```{r}
r_a20[lower.tri(r_a20, diag = T)] <- NA
r_c20[lower.tri(r_c20, diag = T)] <- NA
r_e20[lower.tri(r_e20, diag = T)] <- NA
r_n20[lower.tri(r_n20, diag = T)] <- NA
r_o20[lower.tri(r_o20, diag = T)] <- NA

mean(r_a20, na.rm = T)
mean(r_c20, na.rm = T)
mean(r_e20, na.rm = T)
mean(r_n20, na.rm = T)
mean(r_o20, na.rm = T)
```

```{r}
r_a50[lower.tri(r_a50, diag = T)] <- NA
r_c50[lower.tri(r_c50, diag = T)] <- NA
r_e50[lower.tri(r_e50, diag = T)] <- NA
r_n50[lower.tri(r_n50, diag = T)] <- NA
r_o50[lower.tri(r_o50, diag = T)] <- NA

mean(r_a50, na.rm = T)
mean(r_c50, na.rm = T)
mean(r_e50, na.rm = T)
mean(r_n50, na.rm = T)
mean(r_o50, na.rm = T)
```

```{r}
r_a100[lower.tri(r_a100, diag = T)] <- NA
r_c100[lower.tri(r_c100, diag = T)] <- NA
r_e100[lower.tri(r_e100, diag = T)] <- NA
r_n100[lower.tri(r_n100, diag = T)] <- NA
r_o100[lower.tri(r_o100, diag = T)] <- NA

mean(r_a100, na.rm = T)
mean(r_c100, na.rm = T)
mean(r_e100, na.rm = T)
mean(r_n100, na.rm = T)
mean(r_o100, na.rm = T)
```

# Item-Total Correlations  
Item-total correlations are what they sound like -- the correlation between a single item and a composite (mean) of all other items. Here, for simplicity, we're going to be a little lazy and correlate each item with a composite that includes itself. Because this artificially inflates the correlation, I'll later show you a way to get an estiamte that doesn't include this item.  

## Manual Method  

### IPIP20  
```{r}
(a20_itr <- cor(a20, rowMeans(a20, na.rm = T), use = "pairwise"))
(c20_itr <- cor(c20, rowMeans(c20, na.rm = T), use = "pairwise"))
(e20_itr <- cor(e20, rowMeans(e20, na.rm = T), use = "pairwise"))
(n20_itr <- cor(n20, rowMeans(n20, na.rm = T), use = "pairwise"))
(o20_itr <- cor(o20, rowMeans(o20, na.rm = T), use = "pairwise"))

mean(a20_itr)
mean(c20_itr)
mean(e20_itr)
mean(n20_itr)
mean(o20_itr)
```

### IPIP50  
```{r}
(a50_itr <- cor(a50, rowMeans(a50, na.rm = T), use = "pairwise"))
(c50_itr <- cor(c50, rowMeans(c50, na.rm = T), use = "pairwise"))
(e50_itr <- cor(e50, rowMeans(e50, na.rm = T), use = "pairwise"))
(n50_itr <- cor(n50, rowMeans(n50, na.rm = T), use = "pairwise"))
(o50_itr <- cor(o50, rowMeans(o50, na.rm = T), use = "pairwise"))

mean(a50_itr)
mean(c50_itr)
mean(e50_itr)
mean(n50_itr)
mean(o50_itr)
```

### IPIP100  
```{r}
(a100_itr <- cor(a100, rowMeans(a100, na.rm = T), use = "pairwise"))
(c100_itr <- cor(c100, rowMeans(c100, na.rm = T), use = "pairwise"))
(e100_itr <- cor(e100, rowMeans(e100, na.rm = T), use = "pairwise"))
(n100_itr <- cor(n100, rowMeans(n100, na.rm = T), use = "pairwise"))
(o100_itr <- cor(o100, rowMeans(o100, na.rm = T), use = "pairwise"))

mean(a100_itr)
mean(c100_itr)
mean(e100_itr)
mean(n100_itr)
mean(o100_itr)
```

## `psych` package  
The `psych` package will give us a method for calculating item-total correlations that do not aritificially inflate the correlations. Below, you see the code for doing so using the `alpha()` function from the `psych` package. 

Run the code below for Agreeablness, then repeat for the rest of the Big 5. Once you've done that, do the same for the ipip50 and ipip100.  

### IPIP20  
```{r}
(a20_alpha <- psych::alpha(a20))
(c20_alpha <- psych::alpha(c20))
(e20_alpha <- psych::alpha(e20))
(n20_alpha <- psych::alpha(n20))
(o20_alpha <- psych::alpha(o20))

a20_alpha$item.stats
c20_alpha$item.stats
e20_alpha$item.stats
n20_alpha$item.stats
o20_alpha$item.stats
```

### IPIP50  
```{r}
(a50_alpha <- psych::alpha(a50))
(c50_alpha <- psych::alpha(c50))
(e50_alpha <- psych::alpha(e50))
(n50_alpha <- psych::alpha(n50))
(o50_alpha <- psych::alpha(o50))

a50_alpha$item.stats
c50_alpha$item.stats
e50_alpha$item.stats
n50_alpha$item.stats
o50_alpha$item.stats
```

### IPIP100  
```{r}
(a100_alpha <- psych::alpha(a100))
(c100_alpha <- psych::alpha(c100))
(e100_alpha <- psych::alpha(e100))
(n100_alpha <- psych::alpha(n100))
(o100_alpha <- psych::alpha(o100))

a100_alpha$item.stats
c100_alpha$item.stats
e100_alpha$item.stats
n100_alpha$item.stats
o100_alpha$item.stats
```

What do you notice about the correlations?

# Split-Half Reliability  
Split-half methods are basically a way of testing whether items belong in a scale by looking at whether composites of arbitrary halves of the scale tend to be related to one another. If they are, then this suggests the items are likely to be very related to one another in general.  

Below, you see code for correlating the first and second halves of the Agreeableness scale in the IPIP20. Do the same for the rest of the Big 5 then for the IPIP50 and IPIP100.  

## Manual Method  
### IPIP20  
```{r}
cor(rowMeans(a20[,1:2]), rowMeans(a20[,3:4]))
cor(rowMeans(c20[,1:2]), rowMeans(c20[,3:4]))
cor(rowMeans(e20[,1:2]), rowMeans(e20[,3:4]))
cor(rowMeans(n20[,1:2]), rowMeans(n20[,3:4]))
cor(rowMeans(o20[,1:2]), rowMeans(o20[,3:4]))
```

### IPIP50  
```{r}
cor(rowMeans(a50[,1:5]), rowMeans(a50[,6:10]))
cor(rowMeans(c50[,1:5]), rowMeans(c50[,6:10]))
cor(rowMeans(e50[,1:5]), rowMeans(e50[,6:10]))
cor(rowMeans(n50[,1:5]), rowMeans(n50[,6:10]))
cor(rowMeans(o50[,1:5]), rowMeans(o50[,6:10]))
```

### IPIP100   
```{r}
cor(rowMeans(a100[,1:10]), rowMeans(a100[,11:10]))
cor(rowMeans(c100[,1:10]), rowMeans(c100[,11:10]))
cor(rowMeans(e100[,1:10]), rowMeans(e100[,11:10]))
cor(rowMeans(n100[,1:10]), rowMeans(n100[,11:10]))
cor(rowMeans(o100[,1:10]), rowMeans(o100[,11:10]))
```

Can you think of any reasons why splitting a scale into first and second halves of the scale could be an issue? Can you think of a solution?  

## `psych` package  
The `psych` pacakge has another helpful function, called `splithalf()` that doesn't just choose one arbitrary split of the scale. Instead, it tests all possible splits and gives you the average estimate, which is a more robust estimate.

Below, you see the code for calculating the split-half correlation between items in the Agreeableness scale. Do the same for the rest of the Big 5, then the same for the IPIP50 and IPIP100.  

### IPIP20  
```{r}
splitHalf(a20)
splitHalf(c20)
splitHalf(e20)
splitHalf(n20)
splitHalf(o20)
```

### IPIP50  
```{r}
splitHalf(a50)
splitHalf(c50)
splitHalf(e50)
splitHalf(n50)
splitHalf(o50)
```

### IPIP100  
```{r}
splitHalf(a100)
splitHalf(c100)
splitHalf(e100)
splitHalf(n100)
splitHalf(o100)
```

# Cronbach's alpha  
Finally, the most popular test of internal consistency of a scale is Cronbach's alpha. Basically, what this does is is extend the Spearman Brown prophecy formula, which relies on the split-half correlation to capture all possible split halves.  

The alpha function already calculates this for us, so below, you see code to access that from the results of that function call. Access this for the rest of the Big 5 for the IPIP20, then for the IPIP50 and IPIP100.  

```{r}
a20_alpha$total
c20_alpha$total
e20_alpha$total
n20_alpha$total
o20_alpha$total
```

```{r}
a50_alpha$total
c50_alpha$total
e50_alpha$total
n50_alpha$total
o50_alpha$total
```

```{r}
a100_alpha$total
c100_alpha$total
e100_alpha$total
n100_alpha$total
o100_alpha$total
```

# Scale Development  

