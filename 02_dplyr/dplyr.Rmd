---
title: "Data Manipulation: Intro to `dplyr`"
author: "Emorie D Beck"
output:
  html_document:
    theme: united
    highlight: tango
    df_print: paged
    code_folding: show
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = F)
```

```{r}
library(psych)
library(dplyr)
```

# `dplyr`  
The `dplyr` package is a powerful set of tools from within the larger `tidyverse` suite of functions. `dplyr` functions are useful for a variety of functions, perhaps particularly data manipulation.  

Although there are a large number of functions within the `dplyr` package, today I'm going to introduce a subset of them along with a small number of use cases. But don't worry, these functions will be the key underpinning of code I use in all tutorials going forward, so there will be lots more examples and use cases.  

For now, here's a quick list of the functions we'll cover today:  

1. `filter()`: Pick observations (rows) by their values.  
2. `select()`: Pick variables (columns) by their names.  
3. `arrange()`: Reorder the rows.  
4. `mutate()`: Create new variables with functions of existing variables.  
5. `summarize()` / `summarise()`: Collapse many values down to a single summary.  
6. `group_by()`: Implicitly split the data set by grouping by names (columns).  

Although each of these functions are powerful alone, they are incredibly powerful in conjunction with one another. So below, I'll briefly introduce each function, then link them all together using an example of basic data cleaning and summary.  


# Key `dplyr` Functions  

## `%>%`  
The pipe `%>%` is wonderful. It makes coding intuitive. Often in coding, you need to use so-called nested functions. For example, you might want to round a number after taking the square of 43.  

```{r}
sqrt(43)

round(sqrt(43), 2)
```

The issue with this comes whenever we need to do a series of operations on a data set or other type of object. In such cases, if we run it in a single call, then we have to start in the middle and read our way out.  

The pipe solves this by allowing you to read from left to right (or top to bottom). The easiest way to think of it is that each call of `%>%` reads and operates as "and then." So with the rounded square root of 43, for example: 

```{r}
sqrt(43) %>%
  round(2)
```

As you can see, the two results are the same but the second is slightly easier to follow. And, as you'll see below, this becomes even more intuitive when you start using it in conjunction with `dplyr` functions.  

## `filter()`  
Often times, when conducting research (experiments or otherwise), there are observations (people, specific trials, etc.) that you don't want to include. 

Say for example, that you're interested personality change in adolescence, but you just opened a survey up online. So when you actually download and examine your data, you realize that you have an age range of something like 3-86, not 12-18. In this case, you want to get rid of the people over 18 -- that is, `filter()` them out.  

```{r}
data(bfi) # grab the bfi data from the psych package
bfi <- bfi %>% as_tibble()

summary(bfi$age) # get age descriptives

bfi2 <- bfi %>% # see a pipe!
  filter(age <= 18) # filter to age up to 18

summary(bfi2$age) # summary of the new data 
```

But this isn't quite right. We still have folks below 12. But, the beauty of `filter()` is that you can do sequence of `OR` and `AND` statements when there is more than one condition, such as up to 18 `AND` at least 12.  

```{r}
bfi2 <- bfi %>%
  filter(age <= 18 & age >= 12) # filter to age up to 18 and at least 12

summary(bfi2$age) # summary of the new data 
```

Got it!  

But filter works for more use cases than just conditional `<`, `>`, `<=`, and `>=`. It can also be used for cases where we want a single values to match cases with text. Before I demonstrate that, though, I need to convert one of the variables in the `bfi` data frame to a string. So let's change gender (1 = male, 2 = female) to text (we'll get into factors later).  

```{r}
bfi$education <- plyr::mapvalues(bfi$education, 1:5, c("Below HS", "HS", "Some College", "College", "Higher Degree"))
```

Now let's try a few things: 

1. Create a data set with only individuals with some college (`==`).  

```{r}
bfi2 <- bfi %>% 
  filter(education == "Some College")
unique(bfi2$education)
```

2. Create a data set with only people age 18 (`==`).  

```{r}
bfi2 <- bfi %>%
  filter(age == 18)
summary(bfi2$age)
```

3. Create a data set with individuals with some college or above (`%in%`).  

```{r}
bfi2 <- bfi %>%
  filter(education %in% c("Some College", "College", "Higher Degree"))
unique(bfi2$education)
```

The `%in%` operator is wonderful. Instead of comparing a column to a single value, you can compare it to several. So above, when we wanted ages between 12 and 18, we could have done:  

```{r}
bfi2 <- bfi %>%
  filter(age %in% 12:18)
summary(bfi2$age)
```

I've been using `dplyr` for nearly five years, and I still have to remind myself that when you want to remove rows, you use `filter()`.  

## `select()`  
If `filter()` is for pulling certain observations (rows), then `select()` is for pulling certain variables (columns). Almost without fail, any data that are received for collected are going to have some variables that are not used, not useful, extraneous, etc. In such cases, it's good practice to remove these columns to stop your environment from becoming cluttered and eating up your RAM. 

In our `bfi` data, most of these have been pre-removed, so instead, we'll imagine we don't want to use any indicators of Agreeableness (A1-A5) and that we aren't interested in gender.  

With `select()`, there are few ways choose variables. We can bare quote name the ones we want to keep, bare quote names we want to remove, or use any of a number of `select()` helper functions.  

1. Bare quote columns we want to keep:  
```{r}
bfi %>%
  select(C1, C2, C3, C4, C5)
```

I'm going to stop there because I don't want to name the additional 17 columns we want to keep. Instead we'll use `:` to grab a *range* of columns.  

```{r}
bfi %>%
  select(C1:O5, education, age)
```
2. Bare quote columns we don't want to keep:  

```{r}
bfi %>% 
  select(-(A1:A5), -gender)
```

Note the `()` around the columns. That is necessary when you want to remove a range of columns. 

3. Add or remove using `select()` helper functions.  

* `starts_with("abc")`: matches names that begin with “abc”.  
* `ends_with("xyz")`: matches names that end with “xyz”.  
* `contains("ijk")`: matches names that contain “ijk”.  
* `matches("(.)\\1")`: selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in strings.  
* `num_range("x", 1:3)`: matches x1, x2 and x3.  
* `one_of()`  
* `all_of()`  

## `arrange()` 
Sometimes, either in order to get a better sense of our data or in order to well, order our data, we want to sort it. Although there is a base `R` `sort()` function, the `arrange()` function is `tidyverse` version that plays nicely with other `tidyverse functions`. 

So in our previous examples, we could also `arrange()` our data by age or education, rather than simply filtering. (Or as we'll see later, we can do both!)  

```{r}
# sort by age
bfi %>% 
  select(gender:age) %>%
  arrange(age)

# sort by education
bfi %>%
  select(gender:age) %>%
  arrange(education)
```

## `mutate()`  

## `summarize()` / `summarise()`  

## `group_by()`  

# Bringing it all together: Split-Apply-Combine  
Much of the power of `dplyr` functions lay in the split-apply-combine method. The method is kind of what it sounds like. A given set of of data are *split* into smaller chunks, then a function or series of functions are *applied* to each chunk, and then the chunks are *combined* back together.  
