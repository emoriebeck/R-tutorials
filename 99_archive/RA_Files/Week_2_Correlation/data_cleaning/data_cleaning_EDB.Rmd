---
title: "Data Cleaning"
author: 
  - "Emorie D Beck"
date: "`r Sys.setlocale('LC_TIME', 'C'); format(Sys.time(), '%d\\\\. %B %Y')`"
output:
  html_document:
    theme: united
    highlight: tango
    df_print: paged
    code_folding: show
    toc: true
    toc_float: true
    toc_depth: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction  
This week, we're going to talk about correlations in R. But just as importantly, we are going to get you started on perhaps the most important skill in psychological research -- working with data. Follow along below.  

# Workspace
First, we start by loading the packages and data we'll need.

## Packages  
```{r}
# add packages here. I'd suggest starting with tidyverse, but there are others you 
# may want to add as well
# library(package)

library(psych)
library(plyr)
library(tidyverse)
library(broom)
```

## Load Data
```{r}
# first, let's set the path to the data below. This will be the path to the folder on
# your computer. On a mac, the root directory can be accessed by including "~/" in your
# file path
# example wd <- "~/Dropbox/Summer 2018"
# add your file path below

wd <- "https://github.com/emoriebeck/R-tutorials/blob/master/RA_Files/Week_2_Correlation/data_cleaning"

# now let's load the data. I like to use something like the line below, which uses 
# the `sprintf()` function to merge the file path with the name of the file
# file <- sprintf("%s/data/target_w1.csv", wd)
# pairsW1 <- read.csv(file, stringsAsFactors = F)
# do this below


file <- paste(wd, "home_w1_redacted.csv?raw=true", sep = "/")
file <- url(sprintf("%s/home_w1_redacted.csv?raw=true", wd))
pairsW1 <- file %>% readr::read_csv()

pairsW1
```

# Preparation and Data Cleaning
Now that the data are loaded, we usually need to make modifications, keeping some columns, removing others, creating composites, apply exclusion criteria, etc.  

## Descriptives  
To do this, it's usually smart to look at some descriptives of your data, especially if you aren't very familiar with it. 

One useful function for this is the `describe()` function in the `psych` package  
```{r}
# try using the describe function in the chunk below: 
# describe(pairsW1)
# are there any variables that stand out as weird?
describe(pairsW1)
```

## Selection and Renaming  
When I work with data, I'm usually pulling a few variables from data sets that may contain thousands of variables. I often find the easiest way to do this is to use the codebooks associated with the study to create an Excel sheet that lists a number of properties, including the original column name of the item in the data, a new name that I'd like to give it, the purpose of the variable (e.g. outcome variable, predictor variable, covariate, etc.), whether I need to reverse score it, the scale participants responded on, what values represent missing values (this will vary by data set). See the example I've provided and create your own codebook of variables you find interesting.  
```{r}
# load the codebook
# codebook <- sprintf("%s/codebook.xlsx")
# codebook <- readxl::read_xlsx(codebook)

destfile <- "Codebook.xlsx"
curl::curl_download(sprintf("%s/Codebook.xlsx?raw=true", wd), destfile)
codebook <- readxl::read_excel(destfile)

# Now we need to find a way to match the variables in the codebook with the variables in 
# your data. I'm not going to give you explicit instructions. Struggle a bit, and try 
# different ways. I'll give you a couple of hints below 

# old_names <- codebook$old_names
# new_names <- codebook$new_names

old_names <- codebook$old_name
new_names <- codebook$new_name

# somehow, you need to subset your data so only the variables in your codebook are left 
# in your data frame. Refer to the basics of subsetting data in R (e.g. df[,cols]) and 
# remember what you learned in the intro to tidyverse data camp course

# base R way
pairsW1.base           <- pairsW1[,paste(old_names, "w1", sep = ".")] 
colnames(pairsW1.base) <- new_names

# dplyr way
pairsW1.tidy <- pairsW1 %>% 
  select(one_of(paste(old_names, "w1", sep = "."))) %>%
  setNames(new_names)

# then you need to rename those variables as you've specified in your codebook
# functions like `colnames()` and `setNames()` might be useful here
```

## Handle Missing Values
We want to make sure R recognizes missing values as missing. Below, you're going to need to make sure that missing values are specified as NA, not as another value.
```{r}
# missings are coded as -7 in the data. Change these to NA values.

# base R 
pairsW1.base <- apply(pairsW1.base, 2, function(x) ifelse(x == -7, NA_real_, x))
pairsW1.base <- data.frame(pairsW1.base)

# psych package
pairsW1.psych <- scrub(pairsW1.tidy, new_names, isvalue = -7, newvalue = NA_real_)

# tidyverse
pairsW1.tidy <- pairsW1.tidy %>% mutate_all(funs(mapvalues(., from = -7, to = NA_real_, warn_missing = F)))
```

# Recode Variables  
Almost always, there are variables that are not coded how we want. It could be an ordinal or nominal scale that has levels we don't want or needs reverse coded. It could be we want to collapse variables. Whatever it is, it's important to do.  
```{r}
# recode a nominal variable with 3+ levels into a nominal variable with 2 levels
# try the `recode()` function and also refer to base code techniques  



# now reverse code items that need reverse coded 
# hint: you can create a vector of keys from your codebook
# hint: you could also try moving your data from wide to long format then using full_join to merge your codebook with your data 
# then you can use the psych package function `reverse.code()` to reverse code your data

# psych package
keys <- codebook$rev_code[codebook$rev_code == -1]
items <- codebook$new_name[codebook$rev_code == -1]
pairsW1.psych[,items] <- reverse.code(keys, pairsW1.psych[,items], mini = 1, maxi = 15)

# base R -- don't go there

# tidyverse
items <- (codebook %>% filter(rev_code == -1))$new_name
pairsW1.tidy <- pairsW1.tidy %>%
  gather(key = new_name, value = value, one_of(items), na.rm = T) %>%
  left_join(codebook %>% select(new_name, rev_code, mini, maxi)) %>%
  mutate(value = ifelse(rev_code == -1, reverse.code(-1, value, mini, maxi), value)) %>%
  spread(key = new_name, value = value)
```


## Create composites  
Usually, there are variables that we need to composite. It could be a personality scale, it could be the same item answered over time. Whatever it is, you'll do it a lot. There are lots of ways to go about it. Below, you should try to do this a couple of different ways and see if you achieve the same results.
```{r}
# first, try the dplyr way: get your data in "tidy" format, use the 
# `group_by()` function to group by person, scale, etc., then use the 
# `summarize()` function to create the composites
items <- (codebook %>% filter(scale == "yes"))$new_name

pairsW1.tidy <- pairsW1.tidy %>%
  gather(key = item, value = value, items, na.rm = T) %>%
  separate(item, c("scale", "item"), sep = "_") %>%
  group_by(SID, scale) %>% 
  summarize(value = mean(value, na.rm = T)) %>%
  spread(key = scale, value = value) %>%
  full_join(pairsW1.tidy) %>% 
  ungroup() 

# Now, try the base R way. 
# hint: try using the function `rowMeans()`. You can select multiple columns using 
# the `cbind()` function within `rowMeans()`. Don't forget to set na.rm = T

# personality
items <- codebook$new_name[grepl("BFI.E", codebook$new_name)]
pairsW1.base$BFI.E <- rowMeans(pairsW1.base[,items], na.rm = T)

items <- codebook$new_name[grepl("BFI.A", codebook$new_name)]
pairsW1.base$BFI.A <- rowMeans(pairsW1.base[,items], na.rm = T)

items <- codebook$new_name[grepl("BFI.C", codebook$new_name)]
pairsW1.base$BFI.C <- rowMeans(pairsW1.base[,items], na.rm = T)

items <- codebook$new_name[grepl("BFI.N", codebook$new_name)]
pairsW1.base$BFI.N <- rowMeans(pairsW1.base[,items], na.rm = T)

items <- codebook$new_name[grepl("BFI.O", codebook$new_name)]
pairsW1.base$BFI.O <- rowMeans(pairsW1.base[,items], na.rm = T)

# life satisfaction
items <- codebook$new_name[grepl("^Sat", codebook$new_name)]
pairsW1.base$lifesat <- rowMeans(pairsW1.base[,items], na.rm = T)
```

# Zero-Order Correlations  
```{r}
round(cor(pairsW1.tidy %>% select(BFI.A:Sat), use = "pairwise"),2)

print(corr.test(pairsW1.tidy %>% select(BFI.A:Sat)), short = F)
```

# Regression
```{r}
# Does Extraversion Predict Life Satisfaction  
fitE <- lm(Sat ~ BFI.E, data = pairsW1.tidy)
```

There are lots of helper functions for regression, like: 

`summary()`, which prints a summary of the results  
```{r}
summary(fitE)
```

`coef()`, which prints the coefficients of the model
```{r}
coef(fitE)
```

`residuals()`, which prints the residuals of the model  
```{r}
head(residuals(fitE))
```

`predict()`, which generates predicted values for all the observations of X, using the model  
```{r}
head(predict(fitE))
```

## The tidy solution  
The `broom` package offers a solution for annoying S3 class stuff associated with `lm-class` in `R`.  
It has great functions like:

`tidy()`, which prints a data frame of the coefficients, with the standard errors, t-test statistics associated with the estimates, and p values.  
```{r}
tidy(fitE)
```

`glance()`, which summarizes model fit indices, like $R^2$  
```{r}
glance(fitE)
```

`augment()`, which adds columns with the predictions, residuals, etc. for each data observation  
```{r}
augment(fitE)
```

# Assumptions
## 1. No measurement Error

- influence our ability to control and predict  
- this will never occur with psychological data  
- how bad is it if we don't meet this assumption  
- the better you assess you DV's or IV's, the more you can trust your models  
- you can test this by assessing the reliability of your measures using Cronbach's alpha

Run Cronbach's Alpha - this is actually a really scale to check. We just need something demonstrative. 
```{r}
psych::alpha(pairsW1.tidy %>% select(contains("BFI.E_")))
```

## 2. Correctly specified form

- make sure you are measuring what you think you are measuring?  
- Are you using the correct model?  

```{r}
a_data <- broom::augment(fitE)

ggplot(a_data, aes(.fitted, .resid)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = F) + 
  theme_bw()
```

## 3. No omitted variables  

- there are infinite amount of things  
- this is another assumption you will never meet  
- basically look for hidden moderators and third variables  

## 4. Homoscedasticity   

- conditional variance of residuals aross all levels of X is constant   
- if violated = heteroscedasticity  
- make sure there is constant error variance among all of the X's  
- Residuals v. predict*ors*
```{r}
ggplot(a_data, aes(x = BFI.E, y = .resid)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = F) +
  theme_classic()
```

## 5. Independence among the residuals  

- issues with group, family, longitudinal, and nested designs  
- take care of this with different types of models (e.g ml models)  
- want to see that the residuals don't look different across time  

## 6. Normally Distributed Residuals

```{r}
ggplot(a_data, aes(x = .resid)) + 
  geom_density() + 
  theme_classic()

## Q-Q plot
# dark line is the theoretical normal distribution
# data points for those are close to theoretical
# dotted lines are the confidence bands around it
# want the data points within the confidence bands
# if there are just a couple outside of this, it isn't a huge deal
library(car)
qqPlot(fitE)
```

